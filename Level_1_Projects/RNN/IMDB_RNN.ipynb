{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc62fa3-1d6a-49d9-bdc2-8ca23c009ebb",
   "metadata": {},
   "source": [
    "## IMDb Sentiment Classification using PyTorch RNN\n",
    " ------------------------------------------------\n",
    "### Objective:\n",
    " - Build a Recurrent Neural Network (RNN) for binary sentiment classification\n",
    " - Use IMDb dataset (positive / negative reviews)\n",
    " - Train, evaluate, and summarize model performance\n",
    "\n",
    "### Framework: PyTorch\n",
    "### Model: Embedding + SimpleRNN + Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d8a096-b856-45f5-ab6c-81684cb3320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed843be-4d79-421f-af83-fc83c8a44d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f9a25e12ee4616bfd4772547bf5a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DHARANIDHARAN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DHARANIDHARAN\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91887b3b18024c50b8cfa67cbd28e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d31bd57f90d4240ab476935f973c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377369cd92fa41acadbc7c8b98ba3534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a3924858e2488abd24af67097781ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a895163f8824cefbb974e15818ec282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2696466a2a94a918badea4a25b1ba94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25000, Test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91080019-a669-4e0a-9f75-7e64a5a32323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 51718\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocab(data, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for example in data:\n",
    "        counter.update(tokenizer(example[\"text\"]))\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_data)\n",
    "print(\"Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaa792e-c31a-40ab-8ec7-362fb93794bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab, max_len=200):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens[:max_len]]\n",
    "    ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "        return encode_text(text, self.vocab), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "train_dataset = IMDbDataset(train_data, vocab)\n",
    "test_dataset = IMDbDataset(test_data, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9109e782-0e35-4c59-8c97-35a57e1dd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return self.sigmoid(out).squeeze()\n",
    "\n",
    "model = RNNModel(len(vocab), embed_dim=64, hidden_dim=128).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7041ce14-f7a5-4b07-8f98-062f39446b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████████| 782/782 [15:58<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6968 | Acc: 0.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████████| 782/782 [10:09<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.6925 | Acc: 0.5212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████| 782/782 [15:41<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.6916 | Acc: 0.5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for text, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += ((pred > 0.5) == label).sum().item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Acc: {total_acc/len(train_dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa079601-9bab-4347-8f97-d2cd26ea697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5793\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for text, label in test_loader:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        output = model(text)\n",
    "        pred = (output > 0.5).float()\n",
    "        total += label.size(0)\n",
    "        correct += (pred == label).sum().item()\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3ca02-7e5f-4f69-9918-a0a6bba8e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
